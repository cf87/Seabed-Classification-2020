{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import scipy.io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads KRAKEN data\n",
    "def load_dataset(idx):\n",
    "    filepath = \"files_116/files_VLA_rand_\"+ str(idx) +\"_116.mat\"\n",
    "    \n",
    "    mat = scipy.io.loadmat(filepath)\n",
    "    signal_train=np.array([])\n",
    "  \n",
    "    # Training data\n",
    "    args = (mat['p_cl_n'], mat['p_si_n'], mat['p_sa_n'],mat['p_gr_n'])\n",
    "    for v in args: \n",
    "        tmp=np.vstack([np.real(v), np.imag(v)]) \n",
    "        signal_train=np.hstack([signal_train, tmp]) if signal_train.size else tmp\n",
    "    \n",
    "    \n",
    "    labels_train=np.array([int(np.floor(i/1000)) for i in range(4000)])\n",
    "    \n",
    "    # Test data\n",
    "    signal_test=np.array([])\n",
    "    labels_test=[]\n",
    "    \n",
    "    for j in range(10):\n",
    "        i=j+1\n",
    "        args=(mat[\"p_cl_n\"+str(i)], mat['p_si_n'+str(i)],\n",
    "              mat[\"p_sa_n\"+str(i)], mat[\"p_gr_n\"+str(i)])\n",
    "        for v in args: \n",
    "            tmp=np.vstack([np.real(v), np.imag(v)])\n",
    "            signal_test=np.hstack([signal_test, tmp]) if signal_test.size else tmp\n",
    "\n",
    "        labels_test=np.real(np.append([labels_test], [labels_train]))\n",
    "    \n",
    "    # Test labels are perturbed, associate them with the correct material type\n",
    "    Y=labels_test\n",
    "    labels=labels_test\n",
    "    for i in range(len(labels)):\n",
    "        if abs(labels[i]-1500)<20:\n",
    "            Y[i]=0\n",
    "        else:\n",
    "            if abs(labels[i]-1575)<20:\n",
    "                Y[i]=1\n",
    "            else:\n",
    "                if abs(labels[i]-1650)<20:\n",
    "                    Y[i]=2\n",
    "                else:\n",
    "                    if abs(labels[i]-1800)<20:\n",
    "                        Y[i]=3\n",
    "    \n",
    "    X_train = signal_train.transpose()\n",
    "    y_train = np.array(labels_train, dtype=int);\n",
    "    X_test = signal_test.transpose()\n",
    "    y_test = np.array(Y, dtype=int);\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Benchmark metrics for each classifier\n",
    "# Adapted from Peter Prettenhofer, et. al,\n",
    "# https://scikit-learn.org/0.19/auto_examples/text/document_classification_20newsgroups.html\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "   \n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "  \n",
    "    target_names=[]\n",
    "    targets=np.unique(y);\n",
    "\n",
    "    for i in range(len(targets)):\n",
    "        target_names.append(np.str(targets[i]))\n",
    "    \n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    \n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "    \n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(y_test, pred, target_names=target_names))\n",
    "    \n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    clf_descr = str(str(clf.estimator))\n",
    "    clf_rep = str(metrics.classification_report(y_test, pred, target_names=target_names))\n",
    "    clf_cm = str(metrics.confusion_matrix(y_test, pred))\n",
    "    return clf_descr, score, train_time, test_time, clf_rep, clf_cm\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform, expon\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "# list of (estimator, param_dist), where param_dist is used  in RandomizedSearchCV\n",
    "classifiers = [\n",
    "     (KNeighborsClassifier(), {\n",
    "         'n_neighbors': sp_randint(4, 10),\n",
    "         'weights': ['uniform', 'distance'],\n",
    "         'algorithm':['ball_tree', 'kd_tree'],\n",
    "         'p': [1,2]\n",
    "     }),\n",
    "    (NearestCentroid(), {\n",
    "         'metric': ['euclidean', 'manhattan','minkowski','chebyshev'],\n",
    "         'shrink_threshold': [None, .001, .0001, .1, .01]\n",
    "     }),\n",
    "    (SVC(kernel='linear'), {\n",
    "        'C': np.logspace(-1, 3, 100),\n",
    "        'tol': np.logspace(-8, -2, 100)\n",
    "    }),\n",
    "    (SVC(random_state=42, gamma='scale'), {\n",
    "        'C': [.1, 1, 10, 100, 1000],\n",
    "        'kernel': ['rbf', 'poly', 'linear', 'sigmoid'], \n",
    "        'class_weight':['balanced', None],\n",
    "        'tol': np.logspace(-8, -2, 100)\n",
    "    }),\n",
    "    (MLPClassifier(max_iter=100000), {\n",
    "        'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "        'alpha': expon(scale=.1),\n",
    "        'learning_rate': ['constant','adaptive'],\n",
    "    }),\n",
    "    (DecisionTreeClassifier(),{\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['best','random'],\n",
    "        'max_features':[None, 'auto', 'sqrt', 'log2'],\n",
    "    }),\n",
    "    (RandomForestClassifier(max_depth=5, n_jobs=-1), {\n",
    "        \"max_depth\": sp_randint(2, 20),\n",
    "        \"n_estimators\": sp_randint(2, 50),\n",
    "        \"max_features\": ['auto', 'log2'],\n",
    "        'class_weight': ['balanced', 'balanced_subsample'],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }),\n",
    "    (GaussianProcessClassifier(n_jobs=-1, random_state=42),{\n",
    "        'max_iter_predict': [10, 50, 100],\n",
    "        'multi_class': [\"one_vs_rest\", \"one_vs_one\"],\n",
    "    }),\n",
    "     (GaussianNB(), {\n",
    "         'var_smoothing': np.logspace(-12, -8, 100)\n",
    "     }), \n",
    "     (LinearDiscriminantAnalysis(n_components=2), {\n",
    "         'solver': ['svd', 'lsqr'],\n",
    "         'tol': np.logspace(-10, -2, 100)\n",
    "     }),\n",
    "     (LogisticRegression(solver='newton-cg', random_state=0, max_iter=100000), {\n",
    "        'C': [.1, 1, 10, 100, 1000],\n",
    "         'multi_class': [\"auto\", \"ovr\",\"multinomial\"],\n",
    "         'solver': ['sag', 'saga', 'newton-cg'],\n",
    "         'tol': np.logspace(-6, -4, 100)\n",
    "     }),\n",
    "]\n",
    "names = [e.__class__.__name__ for e, g in classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'files_116/files_VLA_rand_15_116.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'files_116/files_VLA_rand_15_116.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-409eda2ac439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoiselevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mds_cnt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mX0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnoiselevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mds_cnt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_cnt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5e5fc2a1e171>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"files_116/files_VLA_rand_\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"_116.mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msignal_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \"\"\"\n\u001b[1;32m    215\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reader needs file name or open file-like object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'files_116/files_VLA_rand_15_116.mat'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data_name=['KRAKEN 2 layer']\n",
    "dset_name=data_name\n",
    "\n",
    "noiselevel = np.array([15, 17 ,19, 21, 26, 30, 32 ,33, 60])\n",
    "grid_searches = [dict() for x in range(len(noiselevel))]\n",
    "results = [[] for x in range(len(noiselevel))]\n",
    "ds_cnt=-1\n",
    "X0, y0, X_test, y_test= load_dataset(15)\n",
    "for nl in noiselevel:\n",
    "    ds_cnt=ds_cnt+1\n",
    "    X_tmp, y_tmp, X_test, y_test= load_dataset(nl)\n",
    "    \n",
    "    # Split into training and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X0, y0, test_size=.2, random_state=42)\n",
    "\n",
    "    X=X_train\n",
    "    y=y_train\n",
    " \n",
    "    print('\\n Noise level:', str(nl),'\\n')\n",
    "    # preprocess dataset\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for est_idx, (name, (estimator, param_grid)) in \\\n",
    "        enumerate(zip(names, classifiers)):\n",
    "\n",
    "        # Perform randomized grid search over possible hyperparameters\n",
    "        clf = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, n_iter=20, cv=5,\n",
    "                           iid='true', verbose=0, n_jobs=-1)\n",
    "\n",
    "        with ignore_warnings(category=ConvergenceWarning):\n",
    "            results[ds_cnt].append(benchmark(clf))\n",
    "\n",
    "\n",
    "        # Store the classifier\n",
    "        grid_searches[ds_cnt][name] = clf\n",
    "\n",
    "        # Make predictions for the four test cases\n",
    "        print(name)\n",
    "        print('Validation Score', str(clf.score(X_val, y_val)))\n",
    "        print('Test Score', str(clf.score(X_test, y_test)))\n",
    "      \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying results in a pandas dataframe\n",
    "#Code adapted from https://www.kaggle.com/grfiv4/displaying-the-results-of-a-grid-search\n",
    "import pandas as pd\n",
    "def score_summary(grid_searches, sort_by='mean_test_score'):\n",
    "        frames = []\n",
    "        for name, grid_search in grid_searches.items():\n",
    "            frame = pd.DataFrame(grid_search.cv_results_)\n",
    "            frame = frame.filter(regex='^(?!.*param_).*$')\n",
    "            frame['estimator'] = len(frame)*[name]\n",
    "            frames.append(frame)\n",
    "        df = pd.concat(frames)\n",
    "        \n",
    "        df = df.sort_values([sort_by], ascending=False)\n",
    "        df = df.reset_index()\n",
    "        df = df.drop(['rank_test_score', 'index'], 1)\n",
    "        \n",
    "        columns = df.columns.tolist()\n",
    "        columns.remove('estimator')\n",
    "        columns = ['estimator']+columns\n",
    "        df = df[columns]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=score_summary(grid_searches[0])\n",
    "print(noiselevel[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
